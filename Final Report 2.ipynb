{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea59e02f-44be-4bea-a928-28a2af3dda2e",
   "metadata": {},
   "source": [
    "# Final Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c9293-a788-4d6b-94f7-8b0b66127ba6",
   "metadata": {},
   "source": [
    "|           | 80% IG  | 80% Gini Index | 70% IG  | 70% Gini Index | 60% IG  | 60% Gini Index |\n",
    "|-----------|---------|-----------------|---------|-----------------|---------|-----------------|\n",
    "| **Accuracy** | 0.46   | Gini Index: 0.40 | IG: 0.39 | Gini Index: 0.42 | IG: 0.39 | Gini Index: 0.38 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5e5c-b0c8-4ade-9226-b74b774ef744",
   "metadata": {},
   "source": [
    "## Classification Results: \n",
    "we used two classification algorithms: Information Gain (IG) and Gini Index . we evaluate the two algorithms based on accuracy in different 3 training and testing splits. the 3 training and testing splits were: 80% training 20% testing , 70% training 30% testing,  and 60% training 40% testing .\n",
    "\n",
    "### Information Gain (IG)\n",
    "The best performance was achieved with Information Gain at the 80% training, 20% testing split, where the accuracy was 0.46. \n",
    "as it showen in the table in  70% training 30% testing,  and 60% training 40% testing was the accuracy= 0.39, thats mean the information Gain performs was better with larger training data \n",
    "These results suggest that Information Gain is more efficient in handling the data when provided with a larger training set\n",
    "\n",
    "### Gini Index \n",
    " for 80% training, 20% testing split the accuracy was 0.40 , it is lower than Information Gain's performance= 0.46 for the same split. \n",
    " for 70% training, 30% testing split the accuracy was 0.42 , it is higher than Information Gain's performance= 0.39 for the same split\n",
    " for 60% training, 40% testing split the accuracy for gini index was 0.3 which is the lowest among all splits.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978beb1-b24f-402b-9e15-e1aa183a31d9",
   "metadata": {},
   "source": [
    "Information Gain consistently outperformed Gini Index in terms of accuracy, particularly with the 80% training, 20% testing split, where the accuracy was 0.46.\n",
    "Gini Index performed well in the 70% training, 30% testing split but did not exceed Information Gain's performance in any other split.\n",
    "The best performance was achieved with the 80% training, 20% testing split using Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f1681-16ed-4e59-8691-2adcec18a3d6",
   "metadata": {},
   "source": [
    "## clustering metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0e2d5-9805-46fb-ae9e-4718fe5b2779",
   "metadata": {},
   "source": [
    "|                          |  K=4    |   K=6   |   K=8   |\n",
    "|  ----------------------- |-------  |-------- | --------|\n",
    "| Average Silhouette Score | 0.2052  | 0.1726  |  0.1703 |\n",
    "| Total within-cluster sum of square | 3622.19 | 2944.93 | 2687.70 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ceb027-f183-44ea-a01d-c67b64dc10b1",
   "metadata": {},
   "source": [
    "## Total Within-Cluster Sum of Squares\n",
    "K=8 has the lowest value= 2687.70 , this means the data points are more compact in k=8 than k=4 and k=6, and it is close to the cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1decce-d7de-41a0-aa64-949ff1cc54f7",
   "metadata": {},
   "source": [
    "Average Silhouette Score\n",
    "K=4 has the highest Average Silhouette Score which is 0.2052 which suggests that K=4 results in better-separated clusters than K=6 and K=8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc75dd-fc01-4293-8131-b04a55391acb",
   "metadata": {},
   "source": [
    "## Best K Based on Clustering Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab3bb-7113-4688-9480-afbf2a6f7ed1",
   "metadata": {},
   "source": [
    "K=4 has the highest Average Silhouette Score which is the best it is indicate better-defined clusters with better separation even that  K=8  has the lowest Total within-cluster sum of square. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a31bb2-b935-41a8-b750-b938f4d4fc4b",
   "metadata": {},
   "source": [
    " K=6 and K=8 have the lowest Total within-cluster sum of square but they are not as well-separated as K=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0effd89-b2a6-4dab-91c4-85e403c2b928",
   "metadata": {},
   "source": [
    "# Data Mining Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5611af-527b-4e44-b601-f551a53b5554",
   "metadata": {},
   "source": [
    " we applied classification and clustering techniques to analyze the dataset and address the problem of obesity prediction and pattern discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b8126-f3e7-447d-b9e5-22d7feb57281",
   "metadata": {},
   "source": [
    "## Classification Technique\n",
    "### Technique Used: Decision Tree Classifier\n",
    "why Decision Tree Classifier?\n",
    "Decision Trees were chosen due to their interpretability and ability to handle non-linear relationships in the data. They offer a clear and easy-to-understand structure that breaks down the decision-making process, which makes them ideal for understanding the factors that most influence obesity levels. Additionally, Decision Trees are effective in handling both numerical and categorical data, making them suitable for this classification task.\n",
    "\n",
    "### Implementation in Python:\n",
    "Libraries:<br> \n",
    "scikit-learn: Used for implementing the Decision Tree Classifier, splitting the data, and evaluating the model. <br>\n",
    "\n",
    "steps:<br>\n",
    "defining the target variable: The target attribute was defined as Obesity Level ('NObeyesdad'). <br>\n",
    "\n",
    "model training: a decision Tree Classifier was trained using DecisionTreeClassifier from scikit-learn on the training data.<br>\n",
    "\n",
    "model evaluation: the model's performance was evaluated using accuracy, precision, and recall to assess its effectiveness in predicting obesity levels.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699e9d5-7c3e-421f-9eb2-c867a44a74c4",
   "metadata": {},
   "source": [
    "## Clustering Technique:\n",
    "### Technique Used: K-Means Clustering\n",
    "Why K-Means?\n",
    "K-Means was chosen because it effectively divides the data into clear groups, helping to uncover hidden patterns among individuals. It is fast, efficient, and suitable for the continuous, high-dimensional dataset we are working with.\n",
    "\n",
    "### Implementation in Python:\n",
    "Libraries:<br> \n",
    "scikit-learn: for clustering (KMeans) and evaluation (Silhouette Score).<br>\n",
    "kneed: to determine the optimal number of clusters using the Elbow Method.<br>\n",
    "matplotlib:for visualizing the results (Elbow plot and Silhouette plot) <br>\n",
    "\n",
    "steps:<br>\n",
    "Data was preprocessed and scaled using StandardScaler to standardize features, ensuring that all features are on the same scale for effective clustering.<br>\n",
    "\n",
    "the optimal number of clusters (k) was determined using the Silhouette Score, selecting the K with the highest peak.<br>\n",
    "\n",
    "K-Means clustering was performed using the KMeans method from scikit-learn which groups the data into clusters based on similarity.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22583ad-47ef-4252-ad5a-a1c07db89af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
